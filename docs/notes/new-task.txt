AI Home Lab Roadmap (From Current State to Interactive Assistant)
Summary
You already have the core platform running (HA + MQTT + InfluxDB3 + Grafana + Ollama + Agent) and a guarded AI action bridge implemented.
This roadmap moves you from “works in lab” to “interactive Jarvis-like assistant” in controlled phases with clear acceptance checks.

Scope and Outcome
Keep safety-first architecture: AI can suggest and act only through guardrails.
Add practical interaction channels in this order: MQTT command -> Node-RED control UI -> PC voice -> desktop character.
Add controlled autonomy modes (Suggest / Ask / Auto for whitelisted safety cases).
Keep full observability and auditability in Influx/Grafana.
Public Interfaces / Contracts (Canonical)
MQTT command topic: home/ai/command
Payload: plain text command, e.g. turn off plug 2.
MQTT result topic: home/ai/action_result
Payload JSON shape:
{"status":"executed|rejected|failed","command":"...","action":"turn_on|turn_off|none","outlet":1-4,"entity_id":"switch....","detail":"...","time":<unix_seconds>}
Influx measurements:
mqtt_event, agent_suggestion, agent_action.
Home Assistant service contract:
POST /api/services/switch/turn_on|turn_off with entity_id.
Phase Plan
Phase 1 — Close the Loop on AI Action Bridge
Set HA_TOKEN in docker/.env.
Rebuild/restart only agent.
Validate one turn off plug 2 and one turn on plug 2.
Confirm result topic emits status=executed.
Confirm iox.agent_action rows are written with correct status/detail.
Acceptance checks:

docker logs --tail 30 homelab-agent has no HA_TOKEN is empty.
MQTT result payload shows executed for valid commands.
Plug state actually changes in Home Assistant UI.
Influx query on iox.agent_action returns recent execution rows.
Phase 2 — Guardrail Hardening (Before More Autonomy)
Add strict allowlist for actionable entities (already mapped for plug 1..4; keep only these).
Add rate limit for action commands (default: max 1 command per 2 seconds).
Add cooldown per outlet for risky flip-flop (default: 3 seconds).
Reject ambiguous commands unless explicit confirmation pattern exists.
Add command source tagging (source=node_red|voice|manual) in action log detail.
Acceptance checks:

Burst of commands is throttled and logged as rejected.
Ambiguous prompts are rejected with clear reason.
Only allowlisted entities can execute.
Grafana panel shows executed vs rejected count trend.
Phase 3 — Node-RED “Control Console” (Fastest Usable UX)
Build Node-RED flow: text input -> publish home/ai/command.
Add result subscriber node for home/ai/action_result.
Add small dashboard view: last command, status, detail.
Add quick buttons: Plug 1 ON/OFF ... Plug 4 ON/OFF.
Keep text + button actions both routed through same topic for one audit path.
Acceptance checks:

You can issue on/off from browser at http://localhost:1880.
Results appear in UI within 2 seconds.
All actions still recorded in agent_action.
Phase 4 — Voice Command on PC (No Character Yet)
Add local voice capture app/service on your desktop.
Convert speech-to-text and publish to home/ai/command.
Read back result via TTS from home/ai/action_result.
Add push-to-talk mode first, wake-word later.
Default stack choice:

STT local-first if feasible; fallback cloud STT if latency/accuracy is poor.
Keep action execution in existing agent bridge (do not bypass).
Acceptance checks:

Say “turn off plug 2” and action executes.
Voice feedback announces executed/rejected.
End-to-end latency target: under 3 seconds for simple commands.
Phase 5 — Desktop Character Layer
Build a lightweight desktop UI with character avatar, mic button, transcript, and status bubble.
Character only visualizes state and speaks responses; control path remains MQTT bridge.
Show safety mode badge: Suggest, Ask, Auto.
Add “recent actions” mini log in UI (read from result topic).
Acceptance checks:

Character receives and displays latest command/result in real time.
Voice + click commands both work.
No direct device API calls from UI (all via bridge).
Phase 6 — Autonomy Modes (Jarvis-like Behavior with Limits)
Mode A Suggest: no actions, suggestions only.
Mode B Ask: propose action and require user confirmation.
Mode C Auto: only whitelisted safety rules (example: overload -> turn off affected plug + alert).
Store active mode in one source of truth (HA helper entity or config topic).
Acceptance checks:

Mode changes are auditable.
Same command behaves differently by mode as defined.
Auto mode never executes non-whitelisted command.
Phase 7 — Always-On Hosting
Migrate runtime from daily PC to always-on host (mini PC/NAS).
Preserve same compose and env contracts.
Move backups to scheduled daily job.
Add uptime monitor for core endpoints.
Acceptance checks:

Works after host reboot without manual intervention.
7-day continuous health checks pass.
Backup + restore drill succeeds once.
Test Cases and Scenarios
Happy path: turn on plug 2 -> executed -> state changes -> action logged.
Rejection path: turn on everything -> rejected -> reason logged.
Auth path: invalid HA_TOKEN -> failed, clear detail.
Safety path: simulated overload event -> alert generated and action follows active mode.
Concurrency path: 10 rapid commands -> throttled behavior per policy.
Recovery path: restart agent while system running -> reconnects and resumes.
Defaults and Assumptions
Interaction priority: Node-RED first, then PC voice, then character UI.
Assistant personality UI is out-of-band; control stays MQTT-topic based.
Only plugs 1..4 are controllable by AI at this phase.
Home Assistant remains the only actuator authority.
InfluxDB 3 SQL remains canonical telemetry store.
Google Home integration is optional fallback, not primary UX.
Definition of Done (Roadmap Complete)
You can command devices by text and voice through one guarded bridge.
You can observe every decision/action in Grafana and Influx.
You can switch autonomy mode safely.
System runs 24/7 on always-on host with backup/restore confidence.